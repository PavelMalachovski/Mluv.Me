# Mluv.Me - Code Review & Optimization Suggestions

**Date:** December 7, 2025
**Review Type:** Comprehensive Project Analysis
**Focus Areas:** Optimization, New Technologies, Feature Expansion, Monetization

---

## ðŸ“‹ Executive Summary

Mluv.Me is a well-architected language learning platform with solid foundations in Clean Architecture, modern Python practices, and effective AI integration. This review identifies opportunities for:
- **Performance optimization** through caching and async improvements
- **Feature expansion** with Web UI and enhanced personalization
- **Scalability improvements** via task queues and database optimization
- **Revenue generation** through multiple monetization strategies

---

## ðŸš€ 1. Performance Optimization

### 1.1 Redis Caching Implementation

**Current State:** No caching layer exists. Each request hits the database and potentially OpenAI API.

**Proposal: Multi-Layer Redis Caching Strategy**

#### A. User Data Caching
```python
# Cache user settings and profiles (frequently accessed)
# Key pattern: user:{telegram_id}:profile
# TTL: 1 hour (3600s)
# Expected hit rate: 85-90%

# Cache conversation history
# Key pattern: user:{telegram_id}:history
# TTL: 15 minutes (900s)
# Expected reduction in DB queries: 70%
```

**Benefits:**
- Reduce database load by 60-70%
- Improve response time from ~200ms to ~20ms for cached data
- Lower PostgreSQL connection pool pressure

#### B. OpenAI Response Caching
```python
# Cache GPT-4o responses for identical inputs
# Key pattern: honzik:response:{hash(user_text + settings)}
# TTL: 1 day (86400s)
# Expected savings: $50-200/month in API costs

# Cache Whisper transcriptions
# Key pattern: whisper:transcription:{audio_hash}
# TTL: 7 days
# Expected hit rate for repeated exercises: 15-20%
```

**Benefits:**
- Reduce OpenAI API costs by 15-25%
- Faster responses for common phrases/exercises
- Better handling of API rate limits

#### C. Statistics Caching
```python
# Cache daily stats, leaderboards, user progress
# Key pattern: stats:{user_id}:daily:{date}
# TTL: Until day ends (dynamic)
```

**Implementation Plan:**

**Libraries to use:**
- `redis-py` (v5.0+) - Async Redis client
- `fastapi-cache2` - FastAPI caching decorators with Redis backend

**Code Structure:**
```
backend/
  cache/
    __init__.py
    redis_client.py       # Redis connection pool
    cache_keys.py         # Key naming patterns
    decorators.py         # Custom caching decorators
  services/
    cache_service.py      # High-level caching logic
```

**Configuration additions:**
```python
# backend/config.py
REDIS_URL: str = "redis://localhost:6379/0"
REDIS_MAX_CONNECTIONS: int = 50
CACHE_DEFAULT_TTL: int = 3600
CACHE_ENABLED: bool = True
```

**Estimated Impact:**
- Response time: â†“ 60-70%
- Database load: â†“ 65-75%
- OpenAI costs: â†“ 15-25%
- Server resources: â†“ 40-50%

---

### 1.2 Task Queue System (Celery + Redis)

**Current State:** All operations are synchronous in the request cycle, including:
- Audio transcription (Whisper) - 2-5 seconds
- GPT-4o analysis - 3-8 seconds
- TTS generation - 2-4 seconds
- Total blocking time: **7-17 seconds**

**Proposal: Asynchronous Task Processing**

**Use Cases for Background Tasks:**
1. **Non-critical operations:**
   - Statistics aggregation and analytics
   - Daily challenge processing
   - Email/push notifications
   - Database cleanup and archival

2. **Deferred processing:**
   - Batch TTS generation for common phrases
   - Weekly progress reports
   - Vocabulary list generation

3. **Scheduled tasks:**
   - Streak reset checks (daily at midnight user's timezone)
   - Daily challenge reminders
   - Inactive user re-engagement
   - Database backups

**Implementation with Celery:**

```python
# backend/tasks/celery_app.py
from celery import Celery

celery_app = Celery(
    'mluv_tasks',
    broker='redis://localhost:6379/1',
    backend='redis://localhost:6379/2'
)

celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_expires=3600,
    timezone='UTC',
    enable_utc=True,
)
```

**Example Tasks:**

```python
# backend/tasks/analytics.py
@celery_app.task
def calculate_daily_statistics(user_id: int):
    """Background calculation of user statistics"""
    # Process without blocking API response
    pass

@celery_app.task
def send_streak_reminder(user_id: int):
    """Send notification if user hasn't practiced today"""
    pass

@celery_app.task(rate_limit='10/m')
def aggregate_platform_metrics():
    """Platform-wide analytics (rate-limited)"""
    pass
```

**Benefits:**
- API response time: 7-17s â†’ 0.5-2s (immediate acknowledgment)
- Better user experience (instant feedback)
- Resource optimization (tasks run when capacity available)
- Fault tolerance (task retry on failure)

**Estimated Impact:**
- User-perceived latency: â†“ 85-90%
- Server scalability: â†‘ 3-5x
- Error handling: â†‘ Significantly improved

---

### 1.3 Database Optimization

#### A. Missing Indexes

**Analysis of current schema (`backend/models/`):**

```sql
-- Currently missing critical indexes:

-- Messages table (lesson.py fetches recent messages)
CREATE INDEX CONCURRENTLY idx_messages_user_created
  ON messages(user_id, created_at DESC);

-- Daily stats (gamification.py queries by user + date)
CREATE INDEX CONCURRENTLY idx_daily_stats_user_date
  ON daily_stats(user_id, date DESC);

-- Saved words (frequent lookups)
CREATE INDEX CONCURRENTLY idx_saved_words_user_word
  ON saved_words(user_id, word);

-- Messages full-text search (future feature)
CREATE INDEX CONCURRENTLY idx_messages_user_text_search
  ON messages USING gin(to_tsvector('czech', user_text))
  WHERE user_text IS NOT NULL;
```

**Expected Query Performance:**
- Message history fetch: 120ms â†’ 8ms (**15x faster**)
- Daily stats lookup: 80ms â†’ 5ms (**16x faster**)
- Saved words search: 60ms â†’ 3ms (**20x faster**)

#### B. Connection Pooling Optimization

**Current:** Default SQLAlchemy settings
**Proposal:**

```python
# backend/db/database.py
from sqlalchemy.ext.asyncio import create_async_engine

engine = create_async_engine(
    settings.database_url,
    # Optimized pool settings
    pool_size=20,              # Up from default 5
    max_overflow=10,           # Up from default 10
    pool_timeout=30,           # Connection wait timeout
    pool_recycle=3600,         # Recycle connections hourly
    pool_pre_ping=True,        # Verify connection health
    echo=settings.is_development,
)
```

**Benefits:**
- Handle 3-4x more concurrent requests
- Reduce connection acquisition time
- Better handling of connection failures

#### C. Query Optimization

**Current Inefficiency in `lesson.py`:**

```python
# Current: Multiple queries
user = await user_repo.get_by_telegram_id(user_id)
settings = await settings_repo.get_by_user_id(user.id)
messages = await message_repo.get_recent_by_user(user.id)
```

**Optimized with Joined Loading:**

```python
# Proposed: Single query with eager loading
user = await user_repo.get_by_telegram_id_with_relations(
    telegram_id=user_id,
    include=['settings', 'recent_messages']
)
# 3 queries â†’ 1 query = 60% faster
```

**Add to `repositories.py`:**

```python
from sqlalchemy.orm import selectinload

async def get_by_telegram_id_with_relations(
    self,
    telegram_id: int,
    include: list[str] = None
) -> User | None:
    query = select(User).where(User.telegram_id == telegram_id)

    if include:
        if 'settings' in include:
            query = query.options(selectinload(User.settings))
        if 'recent_messages' in include:
            query = query.options(
                selectinload(User.messages)
                .limit(10)
                .order_by(Message.created_at.desc())
            )

    result = await self.session.execute(query)
    return result.scalar_one_or_none()
```

#### D. Materialized Views for Analytics

**For future Web Dashboard:**

```sql
-- Pre-aggregated user statistics
CREATE MATERIALIZED VIEW user_stats_summary AS
SELECT
    u.id,
    u.telegram_id,
    COUNT(DISTINCT m.id) as total_messages,
    AVG(m.correctness_score) as avg_score,
    MAX(ds.streak) as max_streak,
    SUM(s.count) as total_stars
FROM users u
LEFT JOIN messages m ON m.user_id = u.id
LEFT JOIN daily_stats ds ON ds.user_id = u.id
LEFT JOIN stars s ON s.user_id = u.id
GROUP BY u.id;

-- Refresh hourly via Celery task
CREATE INDEX ON user_stats_summary(telegram_id);
```

**Benefits:**
- Dashboard loads: 2-3s â†’ 100-200ms
- Reduced complex query load on production DB
- Enable real-time analytics without performance hit

---

### 1.4 OpenAI API Optimization

**Current State:** Good retry logic exists in `openai_client.py`, but opportunities remain.

#### A. Batch Processing

```python
# For non-real-time operations, batch multiple requests
async def batch_transcribe_audio(audio_files: list[bytes]) -> list[str]:
    """Process multiple transcriptions in parallel with rate limiting"""
    semaphore = asyncio.Semaphore(5)  # Max 5 concurrent

    async def transcribe_with_limit(audio: bytes):
        async with semaphore:
            return await transcribe_audio(audio)

    return await asyncio.gather(
        *[transcribe_with_limit(audio) for audio in audio_files]
    )
```

#### B. Token Usage Optimization

**Current:** Full conversation history sent each time (up to 10 messages)

**Proposal: Smart Context Management**

```python
def optimize_conversation_history(
    messages: list[Message],
    max_tokens: int = 1500
) -> list[dict]:
    """
    Intelligently truncate conversation history:
    - Keep last 3 messages always (recent context)
    - Summarize older messages if token limit exceeded
    - Prioritize messages with mistakes (learning relevance)
    """
    recent = messages[:3]  # Last 3 full messages

    if estimate_tokens(messages) <= max_tokens:
        return format_all(messages)

    older = messages[3:]
    summary = summarize_older_messages(older)  # GPT-3.5-turbo call

    return format_all(recent) + [summary]
```

**Benefits:**
- Reduce token usage by 30-40%
- Maintain conversation quality
- Save $100-300/month at scale

#### C. Model Selection Strategy

```python
# Use cheaper models for simple tasks
MODEL_STRATEGY = {
    'transcription': 'whisper-1',           # No alternative yet
    'tts': 'tts-1',                         # Keep standard
    'analysis_advanced': 'gpt-4o',          # Complex corrections
    'analysis_simple': 'gpt-3.5-turbo',     # Beginner level
    'summarization': 'gpt-3.5-turbo',       # Token reduction
    'translation': 'gpt-3.5-turbo',         # UI text
}

# Dynamically select based on user level
def get_analysis_model(czech_level: str) -> str:
    if czech_level in ['A1', 'A2']:
        return MODEL_STRATEGY['analysis_simple']  # 10x cheaper
    return MODEL_STRATEGY['analysis_advanced']
```

**Expected Savings:** 40-50% for beginner users (typically 60-70% of user base)

---

### 1.5 Code-Level Optimizations

#### A. Async/Await Improvements

**Current Issue:** Some blocking operations in async context

```python
# backend/routers/lesson.py - Current
audio_bytes = await audio.read()  # Good
user_text = await openai_client.transcribe_audio(audio_bytes)  # Good

# But file I/O is blocking:
with open(f"/tmp/{audio.filename}", "wb") as f:  # BLOCKING!
    f.write(audio_bytes)
```

**Proposed Fix:**

```python
import aiofiles

async with aiofiles.open(f"/tmp/{audio.filename}", "wb") as f:
    await f.write(audio_bytes)
```

**Add to requirements.txt:**
```
aiofiles==23.2.1
```

#### B. Pydantic V2 Optimization

**Current:** Using Pydantic 2.9+ but not leveraging full performance

```python
# Enable model serialization caching
from pydantic import ConfigDict

class LessonProcessResponse(BaseModel):
    model_config = ConfigDict(
        # Performance optimizations
        use_enum_values=True,
        validate_assignment=False,  # Skip validation on assignment
        arbitrary_types_allowed=True,
        # Serialization cache
        ser_json_timedelta='float',
        ser_json_bytes='base64',
    )
```

**Expected:** 20-30% faster serialization/deserialization

#### C. Logging Optimization

**Current:** Verbose logging in production

```python
# backend/main.py - Add log level filtering
import structlog

if settings.is_production:
    # Reduce log volume in production
    structlog.configure(
        processors=[
            structlog.processors.filter_log_level('INFO'),  # Skip DEBUG
            # ... existing processors
        ],
    )
```

**Benefits:**
- Reduce log storage costs by 60-70%
- Improve I/O performance
- Easier log analysis

---

## ðŸŒ 2. New Technologies & Features

### 2.1 Web UI Implementation

**Motivation:**
- Telegram is great for mobile, but desktop learning benefits from richer UI
- Web dashboard enables better progress tracking and analytics
- Opens platform to non-Telegram users

#### Technology Stack Recommendation

**Frontend Framework: Next.js 14+ with React**

**Why Next.js:**
- âœ… Server-side rendering (SSR) for SEO
- âœ… Built-in API routes (can proxy to FastAPI)
- âœ… Excellent TypeScript support
- âœ… File-based routing
- âœ… Image optimization
- âœ… Edge runtime support (faster global access)

**Alternative Considered:**
- Vue 3 + Nuxt: Good, but smaller ecosystem
- SvelteKit: Excellent performance, smaller community
- Pure React (Vite): No SSR, manual setup needed

**UI Component Library: shadcn/ui + Tailwind CSS**
- Modern, accessible components
- Full customization
- TypeScript-first
- No runtime overhead (copy-paste components)

#### Web UI Feature Set

**Phase 1: MVP (4-6 weeks)**

1. **User Dashboard**
   ```
   - Login with Telegram (OAuth 2.0)
   - Overview: streak, stars, level
   - Recent lessons with playback
   - Daily progress chart
   ```

2. **Practice Interface**
   ```
   - Text input mode (alternative to voice)
   - Real-time feedback from HonzÃ­k
   - Vocabulary builder
   - Grammar exercises
   ```

3. **Analytics Page**
   ```
   - Progress over time (charts)
   - Most common mistakes
   - Vocabulary growth
   - Time spent learning
   ```

**Phase 2: Advanced Features (8-10 weeks)**

4. **Interactive Lessons**
   ```
   - Themed conversation topics
   - Role-play scenarios
   - Dialogue practice with HonzÃ­k
   ```

5. **Social Features**
   ```
   - Leaderboards (optional opt-in)
   - Study groups
   - Share achievements
   ```

6. **Admin Panel**
   ```
   - User management
   - Platform statistics
   - Content moderation
   - A/B testing dashboard
   ```

#### Project Structure

```
frontend/
  app/
    (auth)/
      login/
      callback/
    dashboard/
      page.tsx              # Main dashboard
      analytics/
      lessons/
      vocabulary/
    api/                    # Next.js API routes (proxy)
      auth/
      lessons/
  components/
    ui/                     # shadcn/ui components
    features/
      LessonCard.tsx
      ProgressChart.tsx
      VocabularyList.tsx
  lib/
    api-client.ts           # Axios/Fetch wrapper
    auth.ts                 # Auth utilities
    types.ts                # TypeScript types
  public/
    assets/
  styles/
    globals.css
  package.json
  tsconfig.json
  next.config.js
```

#### Backend API Extensions Needed

**New Endpoints for Web:**

```python
# backend/routers/web_auth.py
@router.post("/api/v1/web/auth/telegram")
async def telegram_web_auth(telegram_data: TelegramAuthData):
    """Verify Telegram Login Widget data"""
    pass

# backend/routers/web_lessons.py
@router.post("/api/v1/web/lessons/text")
async def process_text_message(
    text: str,
    user_id: int,
    settings: LessonSettings
):
    """Process text input (no audio) for web users"""
    pass

@router.get("/api/v1/web/lessons/history")
async def get_lesson_history(
    user_id: int,
    page: int = 1,
    limit: int = 20
):
    """Paginated lesson history with audio URLs"""
    pass

# backend/routers/analytics.py
@router.get("/api/v1/analytics/progress")
async def get_progress_analytics(
    user_id: int,
    period: str = "30d"  # 7d, 30d, 90d, all
):
    """Detailed progress analytics for charts"""
    pass

@router.get("/api/v1/analytics/vocabulary")
async def get_vocabulary_stats(user_id: int):
    """Vocabulary growth and retention metrics"""
    pass
```

#### Authentication Flow

**Telegram Login Widget Integration:**

```typescript
// frontend/lib/telegram-auth.ts
interface TelegramUser {
  id: number;
  first_name: string;
  username?: string;
  photo_url?: string;
  auth_date: number;
  hash: string;
}

function verifyTelegramAuth(data: TelegramUser): boolean {
  // Verify hash against bot token
  // Implementation per Telegram docs
  return true;
}

// After verification, create JWT for web session
const response = await fetch('/api/v1/web/auth/telegram', {
  method: 'POST',
  body: JSON.stringify(telegramData)
});

const { access_token } = await response.json();
// Store in httpOnly cookie
```

**Benefits:**
- Seamless authentication for existing Telegram users
- No password management needed
- Secure (verified by Telegram's hash)

#### Technology Dependencies

```json
// frontend/package.json
{
  "dependencies": {
    "next": "^14.0.0",
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "@tanstack/react-query": "^5.0.0",  // Data fetching
    "axios": "^1.6.0",
    "recharts": "^2.10.0",               // Charts
    "tailwindcss": "^3.4.0",
    "@radix-ui/react-*": "^1.0.0",       // UI primitives
    "lucide-react": "^0.300.0",          // Icons
    "zustand": "^4.4.0",                 // State management
    "zod": "^3.22.0"                     // Validation
  }
}
```

#### Deployment Strategy

**Option 1: Vercel (Recommended for Next.js)**
- Zero-config deployment
- Automatic HTTPS
- Global CDN
- Built-in analytics
- Free tier available

**Option 2: Railway.com (Same as backend)**
- Keep everything in one platform
- Shared PostgreSQL
- Simpler billing

**Cost Estimate:**
- Vercel Free â†’ Pro: $20/month
- Custom domain: $12/year
- Total: ~$25/month

---

### 2.2 Enhanced Telegram Authorization & Security

#### A. Session Management

**Current State:** No session expiration or device management

**Proposal: Secure Session System**

```python
# New model: backend/models/session.py
class UserSession(Base):
    __tablename__ = "user_sessions"

    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'))
    session_token = Column(String(255), unique=True, index=True)
    device_type = Column(String(50))  # 'telegram_bot', 'web_app', 'mobile_app'
    device_info = Column(JSON)         # Browser, OS, IP
    created_at = Column(DateTime, default=datetime.utcnow)
    last_activity = Column(DateTime, default=datetime.utcnow)
    expires_at = Column(DateTime)      # 30 days default
    is_active = Column(Boolean, default=True)

    user = relationship("User", back_populates="sessions")
```

**Features:**
- Track active sessions per device
- Web UI to revoke sessions
- Automatic expiration after inactivity
- Security alerts for new device logins

#### B. Two-Factor Authentication (2FA)

**For sensitive operations (Web UI only):**

```python
# backend/models/user_security.py
class UserSecurity(Base):
    __tablename__ = "user_security"

    user_id = Column(Integer, ForeignKey('users.id'), primary_key=True)
    totp_secret = Column(String(32))   # For authenticator apps
    totp_enabled = Column(Boolean, default=False)
    backup_codes = Column(JSON)        # Encrypted backup codes
    last_password_change = Column(DateTime)
```

**Use Cases:**
- Optional for premium users
- Required for payment methods
- Recommended for instructors (future feature)

#### C. Rate Limiting Per User

**Current:** Global rate limiting only

```python
# backend/middleware/rate_limit.py
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(
    key_func=get_remote_address,
    default_limits=["200 per day", "50 per hour"],
    storage_uri="redis://localhost:6379/3"  # Use Redis for distributed rate limiting
)

# Per-endpoint limits
@router.post("/api/v1/lessons/process")
@limiter.limit("30 per hour")  # Prevent abuse
async def process_lesson(...):
    pass
```

**Benefits:**
- Prevent abuse and API costs
- Fair resource allocation
- DDoS protection

---

### 2.3 Advanced Personalization Features

#### A. Adaptive Learning System

**Current:** Static difficulty based on user-selected level

**Proposal: Dynamic Difficulty Adjustment**

```python
# backend/services/adaptive_learning.py
class AdaptiveLearningEngine:
    """
    Analyzes user performance and adjusts HonzÃ­k's behavior
    """

    async def calculate_user_proficiency(
        self,
        user_id: int,
        window_days: int = 7
    ) -> UserProficiency:
        """
        Analyze recent performance:
        - Average correctness score
        - Mistake patterns (grammar vs vocabulary vs pronunciation)
        - Learning velocity (improvement rate)
        - Consistency (streak, practice frequency)
        """
        messages = await self.message_repo.get_recent_by_user(
            user_id,
            days=window_days
        )

        metrics = {
            'avg_score': calculate_average_score(messages),
            'improvement_rate': calculate_trend(messages),
            'weak_areas': identify_weak_areas(messages),
            'optimal_difficulty': suggest_difficulty(messages)
        }

        return UserProficiency(**metrics)

    async def suggest_level_change(self, user_id: int) -> Optional[str]:
        """
        Recommend level up/down based on consistent performance
        """
        proficiency = await self.calculate_user_proficiency(user_id, window_days=14)

        if proficiency.avg_score > 85 and proficiency.improvement_rate > 0.1:
            return "level_up"
        elif proficiency.avg_score < 50 and proficiency.improvement_rate < 0:
            return "level_down"

        return None

    async def generate_personalized_exercises(
        self,
        user_id: int
    ) -> list[Exercise]:
        """
        Create targeted exercises based on weak areas
        """
        proficiency = await self.calculate_user_proficiency(user_id)

        # Focus on areas needing improvement
        if 'grammar' in proficiency.weak_areas:
            return generate_grammar_exercises(proficiency.weak_areas['grammar'])

        # More exercises for weak vocabulary
        if 'vocabulary' in proficiency.weak_areas:
            return generate_vocabulary_exercises(proficiency.weak_areas['vocabulary'])

        return []
```

**New Database Tables:**

```python
# backend/models/learning.py
class UserProficiency(Base):
    """Track detailed user proficiency over time"""
    __tablename__ = "user_proficiency"

    user_id = Column(Integer, ForeignKey('users.id'))
    assessed_at = Column(DateTime, default=datetime.utcnow)

    # Overall metrics
    overall_score = Column(Float)
    improvement_rate = Column(Float)

    # Detailed breakdown
    grammar_score = Column(Float)
    vocabulary_score = Column(Float)
    pronunciation_score = Column(Float)
    fluency_score = Column(Float)

    # Weak areas (JSON)
    weak_grammar_topics = Column(JSON)  # ['cases', 'past_tense']
    weak_vocabulary_categories = Column(JSON)  # ['food', 'travel']

    suggested_level = Column(String(10))

class PersonalizedExercise(Base):
    """Custom exercises generated for user"""
    __tablename__ = "personalized_exercises"

    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'))
    exercise_type = Column(String(50))  # 'grammar', 'vocabulary', 'conversation'
    topic = Column(String(100))
    difficulty = Column(String(10))
    content = Column(JSON)  # Exercise data
    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime, nullable=True)
    score = Column(Float, nullable=True)
```

#### B. Learning Style Preferences

**Extend UserSettings:**

```python
# Add to backend/models/user.py
class UserSettings(Base):
    # ... existing fields ...

    # New personalization fields
    learning_style = Column(String(20), default="balanced")
    # Options: 'visual', 'auditory', 'kinesthetic', 'balanced'

    preferred_topics = Column(JSON)  # ['travel', 'business', 'casual', 'culture']
    focus_areas = Column(JSON)      # ['grammar', 'vocabulary', 'pronunciation']

    exercise_frequency = Column(String(20), default="daily")
    # Options: 'intensive' (multiple/day), 'daily', 'relaxed' (few/week)

    challenge_difficulty = Column(String(20), default="moderate")
    # Options: 'easy', 'moderate', 'hard', 'adaptive'
```

**HonzÃ­k Personality Adaptation:**

```python
# backend/services/honzik_personality.py
def _build_system_prompt(self, settings: UserSettings, proficiency: UserProficiency = None):
    # ... existing code ...

    # Add adaptive sections
    if proficiency and proficiency.weak_grammar_topics:
        prompt += f"\n\nUser struggles with: {', '.join(proficiency.weak_grammar_topics)}. "
        prompt += "Gently focus on these areas in conversation."

    if settings.preferred_topics:
        prompt += f"\n\nUser enjoys topics: {', '.join(settings.preferred_topics)}. "
        prompt += "Try to steer conversation toward these interests."

    if settings.learning_style == 'visual':
        prompt += "\n\nUser learns best visually. Include emoji ðŸ“ and encourage written practice."

    return prompt
```

#### C. Spaced Repetition System (SRS)

**For vocabulary retention:**

```python
# backend/models/word.py
class SavedWord(Base):
    # ... existing fields ...

    # SRS fields
    ease_factor = Column(Float, default=2.5)  # SM-2 algorithm
    interval = Column(Integer, default=1)     # Days until next review
    repetitions = Column(Integer, default=0)
    next_review_date = Column(DateTime, default=datetime.utcnow)
    last_review_score = Column(Integer)       # 0-5 (SM-2 scale)

# backend/services/srs_service.py
class SpacedRepetitionService:
    """
    Implement SM-2 algorithm for vocabulary review
    """

    def calculate_next_review(
        self,
        word: SavedWord,
        score: int  # 0-5
    ) -> tuple[datetime, int, float]:
        """
        Returns: (next_review_date, interval, ease_factor)
        """
        if score >= 3:  # Correct
            if word.repetitions == 0:
                interval = 1
            elif word.repetitions == 1:
                interval = 6
            else:
                interval = round(word.interval * word.ease_factor)

            repetitions = word.repetitions + 1
            ease = word.ease_factor + (0.1 - (5 - score) * (0.08 + (5 - score) * 0.02))
        else:  # Incorrect, reset
            interval = 1
            repetitions = 0
            ease = word.ease_factor

        ease = max(1.3, ease)  # Minimum ease factor
        next_date = datetime.utcnow() + timedelta(days=interval)

        return next_date, interval, ease

    async def get_words_due_for_review(self, user_id: int) -> list[SavedWord]:
        """Get vocabulary due for review today"""
        return await self.word_repo.get_due_words(
            user_id,
            before_date=datetime.utcnow()
        )
```

**Integration with Telegram Bot:**

```python
# bot/handlers/vocabulary.py
@router.message(Command("review"))
async def review_vocabulary_command(message: Message, api_client: APIClient):
    """Daily vocabulary review"""
    words = await api_client.get_words_due_for_review(message.from_user.id)

    if not words:
        await message.answer(_("ðŸŽ‰ No words to review today! Great job!"))
        return

    await message.answer(
        _("ðŸ“š You have {count} words to review. Let's start!").format(count=len(words))
    )

    # Start interactive review session
    for word in words:
        await review_word_interactive(message, word)
```

---

### 2.4 Database Improvements

#### A. Archival Strategy

**Problem:** Message table will grow large (millions of rows)

```python
# New table for old messages
class ArchivedMessage(Base):
    __tablename__ = "archived_messages"
    # Same schema as Message
    # Partitioned by year

# Celery task to archive old messages
@celery_app.task
def archive_old_messages(days_old: int = 90):
    """
    Move messages older than X days to archive table
    - Keeps main table fast
    - Archive can be on cheaper storage
    - Still accessible for analytics
    """
    pass
```

#### B. Soft Deletes

**Current:** Hard deletes for user data

```python
# Add to all major tables
class User(Base):
    # ... existing fields ...
    deleted_at = Column(DateTime, nullable=True)

    # Property for convenience
    @property
    def is_deleted(self) -> bool:
        return self.deleted_at is not None
```

**Benefits:**
- GDPR compliance (data retention)
- Accidental deletion recovery
- Analytics integrity (historical data)

#### C. Read Replicas

**For scaling to 100k+ users:**

```python
# backend/db/database.py
# Primary for writes
primary_engine = create_async_engine(settings.database_url)

# Replica for reads (analytics, dashboards)
replica_engine = create_async_engine(settings.database_replica_url)

# Usage in repositories
class BaseRepository:
    def __init__(self, session: AsyncSession, read_only: bool = False):
        self.session = session
        self.engine = replica_engine if read_only else primary_engine
```

**Railway.com Consideration:**
- Replicas available on higher tiers ($20/month+)
- Worth it at 50k+ active users

#### D. JSON Schema Validation

**Current:** JSON columns (user_settings, message content) lack validation

```python
# backend/models/user.py
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy import CheckConstraint

class UserSettings(Base):
    preferred_topics = Column(JSONB)  # Better performance than JSON

    __table_args__ = (
        CheckConstraint(
            "jsonb_typeof(preferred_topics) = 'array'",
            name="preferred_topics_is_array"
        ),
    )
```

**Benefits:**
- Prevent data corruption
- Enforce structure at DB level
- Better query performance with JSONB

---

## ðŸ’° 3. Monetization Strategies

### 3.1 Freemium Model (Primary Strategy)

#### Free Tier
- 5 voice messages per day
- Basic corrections (A1-B1 level)
- Standard response speed
- Telegram bot access only
- 7-day history

#### Premium Tier - "Mluv.Me Pro" ($9.99/month or $99/year)

**Features:**
- âœ… **Unlimited voice messages**
- âœ… **Advanced corrections** (B2-C2 level support)
- âœ… **Priority processing** (2x faster responses via dedicated queue)
- âœ… **Web dashboard access** with analytics
- âœ… **Spaced repetition system** for vocabulary
- âœ… **Custom learning paths** (business Czech, travel, etc.)
- âœ… **Export progress** (PDF reports, CSV data)
- âœ… **Ad-free experience**
- âœ… **Unlimited history**

#### Premium Plus Tier - "Mluv.Me Expert" ($19.99/month)

**All Pro features, plus:**
- âœ… **1-on-1 monthly video call** with real Czech tutor (30 min)
- âœ… **Personalized curriculum** created by tutor
- âœ… **Advanced speech analysis** (pronunciation scoring, accent training)
- âœ… **Certification preparation** (CCE - Certificate in Czech)
- âœ… **Priority support** (email within 24h)

**Implementation:**

```python
# backend/models/user.py
class User(Base):
    # ... existing fields ...
    subscription_tier = Column(String(20), default="free")
    # Options: 'free', 'pro', 'expert'

    subscription_expires_at = Column(DateTime, nullable=True)
    subscription_status = Column(String(20), default="active")
    # Options: 'active', 'cancelled', 'expired', 'paused'

# Middleware to check subscription
# backend/middleware/subscription.py
from functools import wraps

def require_subscription(tier: str = "pro"):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            user = kwargs.get('current_user')
            if not user.has_active_subscription(tier):
                raise HTTPException(
                    status_code=402,  # Payment Required
                    detail=f"This feature requires {tier.title()} subscription"
                )
            return await func(*args, **kwargs)
        return wrapper
    return decorator

# Usage
@router.post("/api/v1/analytics/detailed")
@require_subscription("pro")
async def get_detailed_analytics(current_user: User):
    pass
```

**Payment Integration Options:**

1. **Stripe (Recommended)**
   - Best for global payments
   - Strong fraud protection
   - Subscription management built-in
   - 2.9% + $0.30 per transaction

   ```python
   # backend/services/payment_service.py
   import stripe

   stripe.api_key = settings.stripe_secret_key

   async def create_subscription(user_id: int, tier: str):
       price_id = settings.stripe_prices[tier]

       customer = stripe.Customer.create(
           email=user.email,
           metadata={"user_id": user_id}
       )

       subscription = stripe.Subscription.create(
           customer=customer.id,
           items=[{"price": price_id}],
           metadata={"user_id": user_id, "tier": tier}
       )

       return subscription
   ```

2. **Telegram Payments API**
   - Native to Telegram
   - Supports multiple providers
   - Lower integration overhead
   - Good for CIS region

   ```python
   # bot/handlers/payments.py
   from aiogram.types import LabeledPrice, PreCheckoutQuery

   @router.message(Command("subscribe"))
   async def subscribe_command(message: Message):
       await message.answer_invoice(
           title="Mluv.Me Pro",
           description="Unlimited messages, advanced features",
           prices=[LabeledPrice(label="Pro Monthly", amount=99900)],  # cents
           provider_token=settings.telegram_payment_token,
           currency="CZK",
           payload="pro_monthly"
       )

   @router.pre_checkout_query()
   async def on_pre_checkout(query: PreCheckoutQuery):
       await query.answer(ok=True)
   ```

**Revenue Projections (Conservative):**

| Month | Users | Conversion | Pro Subs | Expert Subs | MRR    | Annual |
|-------|-------|------------|----------|-------------|--------|--------|
| 6     | 1,000 | 3%         | 30       | 3           | $360   | $4,320 |
| 12    | 5,000 | 5%         | 250      | 25          | $3,000 | $36k   |
| 24    | 20,000| 7%         | 1,400    | 140         | $17k   | $204k  |

**Assumptions:**
- Free â†’ Pro conversion: 5-7% (industry average: 2-5%)
- Pro â†’ Expert upgrade: 10%
- Churn rate: 5-7% monthly (with good onboarding)

---

### 3.2 B2B/B2B2C Model - "Mluv.Me for Teams"

**Target Customers:**
- Language schools (partner offering)
- Corporate training departments
- Universities with Czech programs
- NGOs supporting expats/immigrants

**Pricing Tiers:**

#### Team Plan ($49/month + $9/user)
- Up to 10 users
- Centralized billing
- Team analytics dashboard
- Shared vocabulary lists
- Group challenges

#### Enterprise Plan ($299/month + $7/user)
- Unlimited users
- SSO (Single Sign-On)
- Custom branding
- API access
- Dedicated account manager
- SLA guarantee (99.9% uptime)
- Custom integrations

**Implementation:**

```python
# backend/models/organization.py
class Organization(Base):
    __tablename__ = "organizations"

    id = Column(Integer, primary_key=True)
    name = Column(String(200))
    subscription_tier = Column(String(20))  # 'team', 'enterprise'
    max_users = Column(Integer)
    billing_email = Column(String(255))
    created_at = Column(DateTime, default=datetime.utcnow)

    users = relationship("User", back_populates="organization")

class User(Base):
    # ... existing fields ...
    organization_id = Column(Integer, ForeignKey('organizations.id'), nullable=True)
    organization = relationship("Organization", back_populates="users")

# Backend API for team management
@router.post("/api/v1/orgs/{org_id}/users/invite")
@require_org_admin
async def invite_user_to_org(org_id: int, email: str):
    """Send invitation to join organization"""
    pass

@router.get("/api/v1/orgs/{org_id}/analytics")
@require_org_admin
async def get_org_analytics(org_id: int):
    """Team-wide learning analytics"""
    pass
```

**Revenue Potential:**

- 50 team customers (avg 15 users) = $50k/month
- 10 enterprise customers (avg 100 users) = $100k/month
- **Total B2B ARR potential: ~$1.8M**

---

### 3.3 Marketplace Model

**Concept:** Platform for Czech tutors to offer services

#### Revenue Streams:

1. **Commission on Bookings (15-20%)**
   - Users book 1-on-1 lessons with verified tutors
   - Platform handles scheduling, payments, video calls
   - Integrated with Mluv.Me progress data

2. **Premium Tutor Listings**
   - $29/month for tutors to appear at top of search
   - Featured badge
   - Enhanced profile

3. **Content Creation**
   - Tutors create premium lesson packs ($5-$20)
   - Platform takes 30% commission
   - Packs include: audio dialogues, exercises, cultural notes

**Implementation:**

```python
# backend/models/marketplace.py
class Tutor(Base):
    __tablename__ = "tutors"

    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'))
    bio = Column(Text)
    languages = Column(JSON)
    specializations = Column(JSON)  # ['business', 'exam_prep', 'conversation']
    hourly_rate = Column(Float)
    rating = Column(Float, default=5.0)
    total_lessons = Column(Integer, default=0)
    is_verified = Column(Boolean, default=False)
    is_premium_listed = Column(Boolean, default=False)

class Lesson(Base):
    __tablename__ = "tutor_lessons"

    id = Column(Integer, primary_key=True)
    tutor_id = Column(Integer, ForeignKey('tutors.id'))
    user_id = Column(Integer, ForeignKey('users.id'))
    scheduled_at = Column(DateTime)
    duration_minutes = Column(Integer)
    status = Column(String(20))  # 'scheduled', 'completed', 'cancelled'
    price = Column(Float)
    platform_fee = Column(Float)  # 15-20% commission
    meeting_url = Column(String(255))  # Zoom/Google Meet link
    rating = Column(Integer, nullable=True)
    review = Column(Text, nullable=True)
```

**Revenue Example:**
- 1000 lessons/month at avg $20/hour
- 20% commission = $4,000/month
- 50 premium tutor listings = $1,450/month
- **Total Marketplace MRR: ~$5,500**

---

### 3.4 Affiliate & Partnership Revenue

#### A. Language Learning Resources
Partner with:
- Czech textbook publishers (commission on sales)
- Czech grammar apps (referral fees)
- Czech culture platforms (concerts, events)

**Example:** 5% commission on textbook sales
- 500 purchases/month at $30 = $750/month

#### B. Education Platform Partnerships
- Integrate Mluv.Me into existing LMS (Learning Management Systems)
- White-label solution for language schools
- $1-3 per active student/month

**Potential:** 5 schools with 200 students each = $1,000-$3,000/month

#### C. Certification & Testing
- Offer practice tests for Czech certifications (CCE)
- Partner with testing centers for official tests
- $20 per practice test attempt
- 15% revenue share on official test registrations

---

### 3.5 Advertising (Secondary Revenue)

**For Free Tier Only:**

1. **Non-intrusive ads:**
   - Single sponsored message per day
   - Czech language courses/events
   - Travel to Czech Republic
   - Czech products/services

2. **Sponsored Lessons:**
   - Brands create themed conversation lessons
   - Example: "Coffee shop conversation" sponsored by Czech cafÃ© chain
   - Clearly marked as sponsored

**Revenue Estimate:**
- $5 CPM (cost per thousand impressions)
- 50,000 free users, 2 messages/day average = 3M impressions/month
- **Revenue: ~$15,000/month**

**Important:** Ads only for free users, not for premium subscribers

---

### 3.6 Data & Insights (Ethical & Anonymous)

**Product:** "Czech Language Learning Report"

- Aggregate, anonymized data on Czech learning trends
- Sell to universities, research institutions, language policy orgs
- Annual reports: $5,000-$10,000 per customer
- 5-10 customers = $25,000-$100,000/year

**Compliance:**
- GDPR-compliant
- Opt-in for research participation
- No personal data included
- Full transparency with users

---

### 3.7 Grants & Institutional Funding

**Opportunities:**

1. **EU Language Learning Grants**
   - Erasmus+ Programme
   - European Social Fund
   - Typical: â‚¬50,000-â‚¬500,000 per project

2. **Czech Ministry of Education**
   - Integration programs for foreigners
   - â‚¬20,000-â‚¬100,000 grants

3. **Private Foundations**
   - Language preservation initiatives
   - Educational technology grants

**Strategy:**
- Hire grant writer (1 month consulting: $5,000-$10,000)
- Position as integration tool for expats/immigrants
- Emphasize AI innovation in language learning
- Demonstrate social impact

**Potential:** $50,000-$200,000 in first 2 years

---

### 3.8 Monetization Roadmap & Priorities

**Phase 1 (Months 1-6): Foundation**
- âœ… Implement freemium with Pro tier
- âœ… Integrate Stripe payments
- âœ… Basic usage limits for free tier
- **Target: $5,000 MRR**

**Phase 2 (Months 7-12): Expansion**
- âœ… Launch Web UI (enables better premium features)
- âœ… Expert tier with tutor calls
- âœ… B2B team plan pilot (3-5 customers)
- **Target: $20,000 MRR**

**Phase 3 (Months 13-18): Marketplace**
- âœ… Tutor marketplace beta
- âœ… Premium tutor listings
- âœ… First enterprise customer
- **Target: $50,000 MRR**

**Phase 4 (Months 19-24): Scale**
- âœ… Full marketplace launch
- âœ… Affiliate partnerships
- âœ… Advertising for free tier
- âœ… Grant applications
- **Target: $100,000 MRR**

---

## ðŸ“Š 4. Technical Debt & Refactoring

### 4.1 Current Technical Debt Items

#### A. Missing Type Hints
**Files:** Several functions in `backend/services/` lack full type hints

```python
# Current in correction_engine.py
def process_honzik_response(response, user_text):  # Missing types
    pass

# Should be:
def process_honzik_response(
    response: dict[str, Any],
    user_text: str
) -> ProcessedResponse:
    pass
```

**Fix:** Run `mypy --strict` and address all issues

#### B. Hardcoded Values

```python
# backend/routers/lesson.py
messages = await message_repo.get_recent_by_user(user_id, limit=10)  # Hardcoded

# Should be in config.py
class Settings(BaseSettings):
    conversation_history_limit: int = 10
```

#### C. Error Handling Gaps

```python
# bot/handlers/voice.py
audio_bytes = await audio.download_as_bytes()  # No exception handling

# Should wrap in try-except with specific error types
try:
    audio_bytes = await audio.download_as_bytes()
except TelegramAPIError as e:
    logger.error("Failed to download audio", error=str(e))
    await message.reply(_("âŒ Failed to process audio. Please try again."))
    return
```

#### D. Test Coverage Gaps

**Current Coverage:** ~40% (estimated from test files)
**Target:** 80%+

**Missing tests:**
- `honzik_personality.py` - personality prompt generation
- `gamification.py` - streak calculations
- `bot/handlers/voice.py` - full voice flow
- Integration tests for `/lessons/process` endpoint

---

### 4.2 Code Organization Improvements

#### A. Separate Configuration by Environment

```python
# backend/config/base.py
class BaseSettings:
    """Shared settings"""
    pass

# backend/config/production.py
class ProductionSettings(BaseSettings):
    """Production-specific overrides"""
    logging_level: str = "INFO"
    debug: bool = False

# backend/config/development.py
class DevelopmentSettings(BaseSettings):
    logging_level: str = "DEBUG"
    debug: bool = True
```

#### B. Domain-Driven Design Structure

**Current:** Services are mixed
**Proposed:**

```
backend/
  domain/
    learning/
      services/
        correction_engine.py
        adaptive_learning.py
      models/
        message.py
        proficiency.py
      repositories/
        message_repository.py
    gamification/
      services/
        gamification.py
        challenges.py
      models/
        stats.py
        stars.py
    user/
      services/
        user_service.py
      models/
        user.py
      repositories/
        user_repository.py
```

**Benefits:**
- Clearer domain boundaries
- Easier to scale team (different devs per domain)
- Better testability

#### C. API Versioning

**Current:** `/api/v1/` prefix but no version management

```python
# backend/routers/__init__.py
from fastapi import APIRouter

api_v1 = APIRouter(prefix="/api/v1")
api_v2 = APIRouter(prefix="/api/v2")  # Future breaking changes

# main.py
app.include_router(api_v1)
app.include_router(api_v2)
```

---

## ðŸ”’ 5. Security Improvements

### 5.1 Sensitive Data Protection

#### A. Encrypt Audio Files

**Current:** Audio stored temporarily in `/tmp`
**Risk:** Could be accessed by other processes

```python
# backend/services/encryption_service.py
from cryptography.fernet import Fernet

class EncryptionService:
    def __init__(self):
        self.cipher = Fernet(settings.encryption_key)

    def encrypt_audio(self, audio_bytes: bytes) -> bytes:
        return self.cipher.encrypt(audio_bytes)

    def decrypt_audio(self, encrypted_bytes: bytes) -> bytes:
        return self.cipher.decrypt(encrypted_bytes)

# Usage in lesson.py
encrypted_audio = encryption_service.encrypt_audio(audio_bytes)
# Store encrypted version
```

#### B. Secrets Management

**Current:** Environment variables in `.env`
**Better:** Use secrets manager for production

```python
# For Railway.com deployment
# Use built-in secrets management instead of .env

# For AWS/GCP deployment
from google.cloud import secretmanager

async def get_secret(secret_name: str) -> str:
    client = secretmanager.SecretManagerServiceClient()
    name = f"projects/{project_id}/secrets/{secret_name}/versions/latest"
    response = client.access_secret_version(request={"name": name})
    return response.payload.data.decode("UTF-8")
```

#### C. Input Validation & Sanitization

```python
# backend/schemas/lesson.py
from pydantic import validator, Field

class LessonProcessRequest(BaseModel):
    user_id: int = Field(gt=0)  # Must be positive
    audio: UploadFile

    @validator('audio')
    def validate_audio(cls, v):
        # Check MIME type
        allowed_types = ['audio/ogg', 'audio/mpeg', 'audio/wav']
        if v.content_type not in allowed_types:
            raise ValueError(f"Invalid audio type: {v.content_type}")

        # Check file size (from settings)
        max_size = 10 * 1024 * 1024  # 10 MB
        if v.size > max_size:
            raise ValueError("Audio file too large")

        return v
```

---

### 5.2 API Security

#### A. API Key Authentication for Web

**Current:** No API authentication beyond Telegram verification

```python
# backend/middleware/api_key.py
from fastapi import Security, HTTPException
from fastapi.security import APIKeyHeader

api_key_header = APIKeyHeader(name="X-API-Key")

async def verify_api_key(api_key: str = Security(api_key_header)):
    # For web UI and external integrations
    session = await session_repo.get_by_token(api_key)
    if not session or not session.is_valid():
        raise HTTPException(status_code=403, detail="Invalid API key")
    return session.user
```

#### B. CORS Configuration

**Current:** Permissive CORS in `main.py`

```python
# backend/main.py
from fastapi.middleware.cors import CORSMiddleware

if settings.is_production:
    allowed_origins = [
        "https://app.mluv.me",
        "https://mluv.me"
    ]
else:
    allowed_origins = ["http://localhost:3000"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)
```

#### C. SQL Injection Prevention

**Current:** Using SQLAlchemy ORM (good, prevents most SQL injection)
**Additional:** Validate raw SQL if ever used

```python
# If raw SQL is needed, use parameterized queries
from sqlalchemy import text

# NEVER do this:
query = f"SELECT * FROM users WHERE telegram_id = {user_input}"

# ALWAYS do this:
query = text("SELECT * FROM users WHERE telegram_id = :telegram_id")
result = await session.execute(query, {"telegram_id": user_input})
```

---

## ðŸ§ª 6. Testing Improvements

### 6.1 Expand Test Coverage

**Priority Areas:**

#### A. Unit Tests for Services

```python
# tests/test_services/test_honzik_personality.py
import pytest
from backend.services.honzik_personality import HonzikPersonality
from backend.models.user import UserSettings

@pytest.fixture
def honzik_service():
    return HonzikPersonality()

@pytest.mark.parametrize("czech_level,expected_instruction", [
    ("A1", "very simple"),
    ("C2", "advanced concepts"),
])
def test_system_prompt_adapts_to_level(honzik_service, czech_level, expected_instruction):
    settings = UserSettings(czech_level=czech_level)
    prompt = honzik_service._build_system_prompt(settings)
    assert expected_instruction in prompt.lower()

def test_conversation_history_formatting(honzik_service):
    messages = [...]  # Mock messages
    formatted = honzik_service._format_conversation_history(messages)
    assert len(formatted) == len(messages)
    assert all('role' in msg and 'content' in msg for msg in formatted)
```

#### B. Integration Tests

```python
# tests/integration/test_lesson_flow.py
import pytest
from httpx import AsyncClient

@pytest.mark.integration
async def test_full_lesson_process_flow(
    async_client: AsyncClient,
    test_user,
    test_audio_file
):
    """Test complete voice message processing pipeline"""

    # Upload audio
    files = {"audio": ("test.ogg", test_audio_file, "audio/ogg")}
    response = await async_client.post(
        f"/api/v1/lessons/process?user_id={test_user.telegram_id}",
        files=files
    )

    assert response.status_code == 200
    data = response.json()

    # Verify all components
    assert "honzik_text" in data
    assert "honzik_response_audio" in data
    assert "stars_earned" in data
    assert data["stars_earned"] >= 0

    # Verify DB changes
    messages = await message_repo.get_recent_by_user(test_user.id)
    assert len(messages) >= 2  # User message + Honzik response
```

#### C. Load Testing

```python
# tests/load/locustfile.py
from locust import HttpUser, task, between

class MluvMeUser(HttpUser):
    wait_time = between(1, 5)

    @task(3)
    def process_voice_message(self):
        with open("test_audio.ogg", "rb") as audio:
            self.client.post(
                "/api/v1/lessons/process?user_id=12345",
                files={"audio": audio}
            )

    @task(1)
    def get_user_stats(self):
        self.client.get("/api/v1/stats/12345")

# Run: locust -f tests/load/locustfile.py --users 100 --spawn-rate 10
```

**Target Metrics:**
- 500 concurrent users
- 95th percentile response time < 3 seconds
- 0% error rate

---

### 6.2 Mocking External Services

```python
# tests/mocks/openai_mock.py
from unittest.mock import AsyncMock

class MockOpenAIClient:
    """Mock OpenAI API for testing without API costs"""

    async def transcribe_audio(self, audio: bytes) -> str:
        return "DobrÃ½ den, jak se mÃ¡te?"

    async def generate_chat_completion(
        self,
        messages: list[dict],
        **kwargs
    ) -> dict:
        return {
            "honzik_response": "SkvÄ›le! Jak se mÃ¡te vy?",
            "user_mistakes": [],
            "suggestions": ["Great pronunciation!"],
            "correctness_score": 95
        }

    async def generate_speech(self, text: str, voice: str) -> bytes:
        # Return dummy audio bytes
        return b"MOCK_AUDIO_DATA"

# Usage in tests
@pytest.fixture
def mock_openai(monkeypatch):
    mock = MockOpenAIClient()
    monkeypatch.setattr("backend.services.openai_client.OpenAIClient", mock)
    return mock
```

---

## ðŸ“ˆ 7. Monitoring & Observability

### 7.1 Application Performance Monitoring (APM)

**Recommendation: Sentry**

```python
# backend/main.py
import sentry_sdk
from sentry_sdk.integrations.fastapi import FastApiIntegration
from sentry_sdk.integrations.sqlalchemy import SqlalchemyIntegration

sentry_sdk.init(
    dsn=settings.sentry_dsn,
    environment=settings.environment,
    integrations=[
        FastApiIntegration(),
        SqlalchemyIntegration(),
    ],
    traces_sample_rate=1.0 if settings.is_development else 0.1,
    profiles_sample_rate=0.1,
)
```

**Benefits:**
- Error tracking and alerting
- Performance monitoring
- User impact analysis
- Release tracking

**Cost:** Free up to 5k events/month, $26/month for 50k events

---

### 7.2 Metrics & Dashboards

**Recommendation: Prometheus + Grafana**

```python
# backend/middleware/metrics.py
from prometheus_client import Counter, Histogram, Gauge

# Define metrics
request_count = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

active_users = Gauge(
    'active_users_total',
    'Number of active users'
)

# OpenAI metrics
openai_requests = Counter(
    'openai_requests_total',
    'OpenAI API requests',
    ['model', 'status']
)

openai_token_usage = Counter(
    'openai_tokens_total',
    'OpenAI tokens used',
    ['model', 'type']  # 'prompt' or 'completion'
)

# Expose metrics endpoint
from prometheus_client import generate_latest

@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type="text/plain")
```

**Key Metrics to Track:**

1. **API Performance:**
   - Request rate (requests/sec)
   - Response time (p50, p95, p99)
   - Error rate (4xx, 5xx)

2. **Business Metrics:**
   - Daily active users (DAU)
   - Messages processed/day
   - Conversion rate (free â†’ paid)
   - Churn rate

3. **OpenAI Usage:**
   - API calls/hour
   - Token consumption
   - Cost per user
   - Rate limit hits

4. **Database:**
   - Query duration
   - Connection pool usage
   - Slow queries (>100ms)

---

### 7.3 Logging Enhancements

**Current:** Good structlog setup
**Add:** Log aggregation and search

**Option 1: ELK Stack (Elasticsearch, Logstash, Kibana)**
- Self-hosted or Elastic Cloud
- Powerful search and analytics
- Cost: Free (self-hosted) or $95/month (cloud)

**Option 2: Railway Logs + Datadog**
- Native Railway integration
- Real-time log streaming
- Cost: $15/month (Datadog free tier)

```python
# Enhanced logging with context
import structlog

logger = structlog.get_logger()

# Add user context to all logs in a request
@app.middleware("http")
async def add_user_context(request: Request, call_next):
    user_id = request.headers.get("X-User-ID")
    structlog.contextvars.bind_contextvars(user_id=user_id)
    response = await call_next(request)
    structlog.contextvars.unbind_contextvars("user_id")
    return response

# Usage
logger.info("lesson_processed",
    correctness_score=85,
    stars_earned=10,
    duration_ms=1500
)
# Output: {"event": "lesson_processed", "user_id": "12345", "correctness_score": 85, ...}
```

---

## ðŸš€ 8. Implementation Roadmap

### Phase 1: Quick Wins (2-4 weeks)

**Priority: High Impact, Low Effort**

1. âœ… **Redis Caching** (1 week)
   - User data caching
   - Basic stats caching
   - Immediate 60% latency reduction

2. âœ… **Database Indexes** (2 days)
   - Add missing indexes
   - Optimize queries
   - 10-15x query speedup

3. âœ… **Async File I/O** (1 day)
   - Add `aiofiles`
   - Fix blocking operations
   - Minor performance gain

4. âœ… **Monitoring Setup** (3 days)
   - Sentry integration
   - Basic Prometheus metrics
   - Error tracking

**Estimated Impact:**
- Response time: â†“ 65%
- Error visibility: â†‘ 100%
- Team velocity: â†‘ 20% (better debugging)

---

### Phase 2: Monetization Foundation (6-8 weeks)

**Priority: Revenue Generation**

1. âœ… **Freemium Implementation** (2 weeks)
   - Subscription tiers in DB
   - Usage limits for free tier
   - Stripe integration
   - Payment webhooks

2. âœ… **Web UI MVP** (4 weeks)
   - Next.js setup
   - Telegram auth
   - Dashboard page
   - Lessons history
   - Analytics charts

3. âœ… **Enhanced Features for Premium** (2 weeks)
   - Advanced corrections
   - Spaced repetition
   - Export functionality
   - Priority queue

**Estimated Revenue:** $5,000 MRR by end of phase

---

### Phase 3: Scale & Optimize (8-12 weeks)

**Priority: Performance at Scale**

1. âœ… **Celery Task Queue** (2 weeks)
   - Setup Celery + Redis
   - Move non-critical tasks to background
   - Implement scheduled tasks
   - Monitoring and retry logic

2. âœ… **Advanced Caching** (1 week)
   - OpenAI response caching
   - Audio caching
   - Cache invalidation strategy

3. âœ… **Database Optimization** (2 weeks)
   - Query optimization with eager loading
   - Connection pooling tuning
   - Archival system for old data
   - Materialized views

4. âœ… **Load Testing & Optimization** (1 week)
   - Locust load tests
   - Identify bottlenecks
   - Optimize critical paths
   - Target: 1000 concurrent users

5. âœ… **Comprehensive Testing** (2 weeks)
   - Increase coverage to 80%
   - Integration tests
   - E2E tests for critical flows

**Estimated Impact:**
- Handle 10x more users
- 50% reduction in infrastructure costs per user
- 99.9% uptime

---

### Phase 4: Advanced Features & B2B (12-16 weeks)

**Priority: Market Expansion**

1. âœ… **Adaptive Learning System** (3 weeks)
   - User proficiency tracking
   - Dynamic difficulty adjustment
   - Personalized exercises
   - ML model training (optional)

2. âœ… **B2B Team Platform** (4 weeks)
   - Organization model
   - Team management UI
   - Centralized billing
   - Team analytics

3. âœ… **Tutor Marketplace MVP** (4 weeks)
   - Tutor profiles
   - Booking system
   - Video call integration (Zoom/Google Meet)
   - Payment processing

4. âœ… **Mobile App** (6 weeks)
   - React Native or Flutter
   - Same backend APIs
   - Push notifications
   - Offline mode

**Estimated Revenue:** $50,000 MRR

---

## ðŸ’¡ 9. Additional Recommendations

### 9.1 AI/ML Enhancements

#### A. Fine-Tuned Model for Czech

**Current:** Using general GPT-4o
**Opportunity:** Fine-tune on Czech language teaching data

```python
# Collect training data from successful lessons
# Format: user input â†’ ideal HonzÃ­k response
training_data = [
    {
        "messages": [
            {"role": "system", "content": HONZIK_SYSTEM_PROMPT},
            {"role": "user", "content": "JÃ¡ chci jÃ­t do obchod."},
            {"role": "assistant", "content": json.dumps({
                "honzik_response": "RozumÃ­m! SprÃ¡vnÄ› by to mÄ›lo bÃ½t 'Chci jÃ­t do obchodu.' ...",
                "user_mistakes": ["do obchod â†’ do obchodu (genitiv)"],
                "suggestions": [...],
                "correctness_score": 75
            })}
        ]
    },
    # ... thousands more examples
]

# Fine-tune via OpenAI API
import openai

openai.FineTuningJob.create(
    training_file="file-abc123",
    model="gpt-4o-2024-08-06",
    hyperparameters={
        "n_epochs": 3
    }
)
```

**Benefits:**
- Better Czech grammar understanding
- More consistent corrections
- Potentially lower cost per request
- Faster responses

**Cost:** $8/1M tokens training + usage costs (similar to base model)

#### B. Speech Pronunciation Scoring

**Beyond transcription, analyze pronunciation quality:**

```python
# Use specialized model (e.g., wav2vec2)
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor

class PronunciationScorer:
    def __init__(self):
        self.processor = Wav2Vec2Processor.from_pretrained(
            "facebook/wav2vec2-xls-r-300m"
        )
        self.model = Wav2Vec2ForCTC.from_pretrained(
            "finetuned-czech-pronunciation"  # Would need to fine-tune
        )

    async def score_pronunciation(
        self,
        audio: bytes,
        expected_text: str
    ) -> PronunciationScore:
        """
        Returns scores for:
        - Overall accuracy (0-100)
        - Individual phoneme scores
        - Common mistakes (e.g., "r" vs "Å™")
        """
        pass
```

**Use Case:** Premium feature for Expert tier

---

### 9.2 User Experience Improvements

#### A. Onboarding Flow

**Current:** Basic command-based onboarding
**Improved:**

```python
# Multi-step interactive onboarding
async def onboarding_flow(message: Message):
    """
    Step 1: Welcome + explain HonzÃ­k
    Step 2: Level assessment (ask user to speak 30 seconds)
    Step 3: Goal setting (travel, work, exam, general)
    Step 4: Personalization (topics, learning pace)
    Step 5: First conversation with HonzÃ­k
    """

    # Automatically assess level from initial recording
    assessment = await assess_czech_level(audio)
    await user_repo.update(user_id, czech_level=assessment.level)

    await message.answer(
        f"Based on your speech, I'd place you at {assessment.level} level. "
        f"Does that sound right? (Yes/No)"
    )
```

**Benefits:**
- Better engagement
- More accurate personalization
- Higher retention

#### B. Gamification Enhancements

**New Features:**

1. **Achievements System**
   ```python
   # backend/models/gamification.py
   class Achievement(Base):
       id = Column(Integer, primary_key=True)
       code = Column(String(50), unique=True)  # 'first_week', 'perfect_score'
       title_key = Column(String(100))
       description_key = Column(String(200))
       icon = Column(String(50))
       points = Column(Integer)

   class UserAchievement(Base):
       user_id = Column(Integer, ForeignKey('users.id'))
       achievement_id = Column(Integer, ForeignKey('achievements.id'))
       unlocked_at = Column(DateTime, default=datetime.utcnow)
   ```

2. **Leaderboards**
   ```python
   # Weekly/Monthly leaderboards
   @router.get("/api/v1/leaderboard")
   async def get_leaderboard(period: str = "week", limit: int = 100):
       """
       Top users by:
       - Total stars
       - Longest streak
       - Most improved (score delta)
       """
       pass
   ```

3. **Social Sharing**
   ```python
   # Generate shareable achievement images
   from PIL import Image, ImageDraw, ImageFont

   def create_achievement_image(user: User, achievement: Achievement) -> bytes:
       """Create social media image for achievement"""
       img = Image.new('RGB', (1200, 630), color=(73, 109, 137))
       # Add text, icons, user stats
       return img_bytes

   # Share to Telegram Stories or other platforms
   ```

#### C. Progress Insights

**AI-powered learning advice:**

```python
# Weekly progress email/notification
async def generate_weekly_insights(user_id: int) -> str:
    """
    Analyze user's week and provide personalized insights:
    - What went well
    - Areas to focus on
    - Suggested exercises
    - Motivational message
    """
    messages = await get_last_week_messages(user_id)
    proficiency = await adaptive_learning.calculate_user_proficiency(user_id, 7)

    prompt = f"""
    Generate encouraging weekly progress report for Czech learner:
    - Completed {len(messages)} lessons
    - Average score: {proficiency.avg_score}
    - Improvement: {proficiency.improvement_rate}%
    - Weak areas: {proficiency.weak_areas}

    Tone: Encouraging, specific, actionable
    """

    insights = await openai_client.generate_chat_completion([
        {"role": "system", "content": "You are a supportive Czech teacher."},
        {"role": "user", "content": prompt}
    ])

    return insights
```

---

### 9.3 Content & Community

#### A. Themed Lesson Packs

**Pre-created conversation templates:**

```python
# backend/models/content.py
class LessonPack(Base):
    __tablename__ = "lesson_packs"

    id = Column(Integer, primary_key=True)
    title = Column(String(200))
    description = Column(Text)
    theme = Column(String(50))  # 'travel', 'business', 'dating', 'medical'
    difficulty = Column(String(10))
    is_premium = Column(Boolean, default=False)

    lessons = relationship("PackLesson", back_populates="pack")

class PackLesson(Base):
    __tablename__ = "pack_lessons"

    id = Column(Integer, primary_key=True)
    pack_id = Column(Integer, ForeignKey('lesson_packs.id'))
    order = Column(Integer)
    title = Column(String(200))
    scenario = Column(Text)  # "You're at a restaurant ordering food..."
    vocabulary = Column(JSON)  # Key words for this lesson
    expected_phrases = Column(JSON)
```

**Example Packs:**
- ðŸ¥ "At the Doctor" (10 lessons) - $4.99
- âœˆï¸ "Travel Essentials" (15 lessons) - $6.99
- ðŸ’¼ "Business Czech" (20 lessons) - $9.99

**Revenue:** Micropayments, ~$2,000/month at scale

#### B. Community Features

**User forum/Discord:**
- Study tips sharing
- Progress celebration
- Q&A with tutors (for premium users)
- Language exchange matching

**Benefits:**
- User retention
- Organic growth (word of mouth)
- User-generated content
- Reduced support burden

---

## ðŸ“‹ 10. Summary & Next Actions

### Top 10 Priorities (Ranked by Impact)

1. **Redis Caching** âš¡
   - **Impact:** 65% latency reduction
   - **Effort:** 1 week
   - **ROI:** Immediate, massive

2. **Freemium + Stripe** ðŸ’°
   - **Impact:** Revenue generation
   - **Effort:** 2 weeks
   - **ROI:** $5k MRR in 3-6 months

3. **Database Indexes** ðŸ”
   - **Impact:** 10-15x query speedup
   - **Effort:** 2 days
   - **ROI:** Instant performance boost

4. **Web UI MVP** ðŸŒ
   - **Impact:** Market expansion, premium justification
   - **Effort:** 4 weeks
   - **ROI:** 2x conversion rate

5. **Celery Task Queue** â±ï¸
   - **Impact:** 85% faster perceived response
   - **Effort:** 2 weeks
   - **ROI:** Better UX, scalability

6. **Monitoring (Sentry)** ðŸ“Š
   - **Impact:** 100% error visibility
   - **Effort:** 3 days
   - **ROI:** Faster debugging, fewer incidents

7. **Adaptive Learning** ðŸ§ 
   - **Impact:** Better outcomes, retention
   - **Effort:** 3 weeks
   - **ROI:** 20% higher retention

8. **B2B Team Plan** ðŸ‘¥
   - **Impact:** High-value customers
   - **Effort:** 4 weeks
   - **ROI:** $50k-100k ARR potential

9. **Test Coverage to 80%** ðŸ§ª
   - **Impact:** Confidence, velocity
   - **Effort:** 2 weeks
   - **ROI:** Fewer bugs, faster shipping

10. **Tutor Marketplace** ðŸ‘¨â€ðŸ«
    - **Impact:** Platform business model
    - **Effort:** 4 weeks
    - **ROI:** $5k-10k MRR

---

### Immediate Next Steps (This Week)

#### Day 1-2: Redis Caching Setup
```bash
# Install Redis
pip install redis aioredis fastapi-cache2

# Update docker-compose.yml (for local dev)
services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

# Add to railway.json
{
  "services": [
    {
      "name": "redis",
      "plan": "starter",  # $5/month
      "image": "redis:7-alpine"
    }
  ]
}
```

Create `backend/cache/redis_client.py` and implement user data caching first.

#### Day 3-4: Database Indexes
```sql
-- Run these migrations
CREATE INDEX CONCURRENTLY idx_messages_user_created ON messages(user_id, created_at DESC);
CREATE INDEX CONCURRENTLY idx_daily_stats_user_date ON daily_stats(user_id, date DESC);
CREATE INDEX CONCURRENTLY idx_saved_words_user_word ON saved_words(user_id, word);
```

#### Day 5: Monitoring
```bash
pip install sentry-sdk

# Add to main.py
import sentry_sdk
sentry_sdk.init(dsn=settings.sentry_dsn)
```

Create Sentry account (free tier) and configure.

---

### Cost-Benefit Analysis

| Initiative | Development Cost | Monthly Op Cost | Monthly Revenue | Payback Period |
|------------|------------------|-----------------|-----------------|----------------|
| Redis Caching | $2,000 (1 week) | $5 | $0 (saves infra) | Immediate |
| Freemium | $4,000 (2 weeks) | $20 (Stripe) | $5,000+ | 1 month |
| Web UI | $8,000 (4 weeks) | $25 (Vercel) | $10,000+ | 1 month |
| Celery | $4,000 (2 weeks) | $5 (Redis) | $0 (saves infra) | 2 months |
| B2B Platform | $8,000 (4 weeks) | $0 | $5,000+ | 2 months |
| Marketplace | $12,000 (6 weeks) | $50 | $5,000+ | 3 months |
| **Total Year 1** | **$38,000** | **$110/mo** | **$30k-50k/mo** | **2-3 months** |

**Assumptions:**
- Developer rate: $50-100/hour or one full-time engineer
- Conservative revenue estimates
- Gradual rollout over 12 months

---

### Technology Stack Summary

**New Technologies to Add:**

| Technology | Purpose | Priority | Cost |
|------------|---------|----------|------|
| Redis | Caching, task queue | HIGH | $5-10/mo |
| Celery | Background tasks | HIGH | $0 (uses Redis) |
| Next.js 14 | Web frontend | HIGH | $0 (open source) |
| Stripe | Payments | HIGH | 2.9% + $0.30/txn |
| Sentry | Error tracking | MEDIUM | $0-26/mo |
| Prometheus + Grafana | Metrics | MEDIUM | $0 (self-hosted) |
| aiofiles | Async file I/O | LOW | $0 |
| Locust | Load testing | LOW | $0 |

---

## ðŸŽ¯ Conclusion

Mluv.Me is a **solid foundation** with significant growth potential. The project demonstrates:
- âœ… Strong technical architecture
- âœ… Effective AI integration
- âœ… Clear value proposition
- âœ… Engaged target market

**Key Opportunities:**
1. **Performance**: 3-5x improvement possible with caching and async optimization
2. **Revenue**: $100k+ MRR achievable within 18-24 months via freemium, B2B, and marketplace
3. **Scale**: Can support 100k+ users with proposed infrastructure improvements
4. **Differentiation**: Adaptive learning and personalization set apart from competitors

**Recommended Focus:**
- **Months 1-3:** Performance optimization (Redis, indexes, monitoring)
- **Months 4-6:** Monetization (freemium, Web UI, premium features)
- **Months 7-12:** Scale (B2B, marketplace, mobile app)
- **Months 13-24:** Market expansion (international, enterprise, partnerships)

**Success Metrics to Track:**
- DAU/MAU ratio (target: >25%)
- Free â†’ Paid conversion (target: 5-7%)
- Monthly churn (target: <7%)
- NPS score (target: >50)
- ARR growth (target: 100% YoY)

---

## ðŸ“š References & Resources

### Learning Resources
- FastAPI Caching: https://github.com/long2ice/fastapi-cache
- Celery Best Practices: https://docs.celeryq.dev/en/stable/userguide/tasks.html
- Next.js Documentation: https://nextjs.org/docs
- Stripe Integration Guide: https://stripe.com/docs/api

### Benchmarking
- Language learning app metrics: https://www.duolingo.com/approach
- SaaS pricing strategies: https://www.priceintelligently.com/
- Freemium conversion rates: https://www.profitwell.com/recur/all/freemium-conversion-rates

### Tools
- Load testing: https://locust.io/
- Monitoring: https://sentry.io/
- Analytics: https://www.mixpanel.com/

---

**Document Version:** 1.0
**Author:** AI Code Review Assistant
**Date:** December 7, 2025
**Status:** Ready for Implementation
